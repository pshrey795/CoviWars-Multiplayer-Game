{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCpYmeN1lOcz"
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8rFaTvFab6Kz"
   },
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYzXFNp7oLiR",
    "outputId": "6fd91255-4c39-476c-a9c3-c5a789f645bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-retro in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-retro) (0.15.7)\n",
      "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from gym-retro) (1.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-retro\n",
    "# !pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHB7DA_WHkEm",
    "outputId": "3231916c-4651-4366-b860-a6d17a24e268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/baselines\n",
      "  Cloning https://github.com/openai/baselines to /tmp/pip-req-build-xixlplo1\n",
      "  Running command git clone -q https://github.com/openai/baselines /tmp/pip-req-build-xixlplo1\n",
      "Requirement already satisfied (use --upgrade to upgrade): baselines==0.1.6 from git+https://github.com/openai/baselines in /usr/local/lib/python3.7/dist-packages\n",
      "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (0.15.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.41.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.0.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.2.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (7.1.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
      "Building wheels for collected packages: baselines\n",
      "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for baselines: filename=baselines-0.1.6-cp37-none-any.whl size=220664 sha256=df810967a34b9b286e643cd7c181ac912cff9ad5c7794cba7e9943d5a1f60389\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vykx5xt2/wheels/d8/55/79/53870462a2ec11a80a4edf582cd45f173da693fe317190ee29\n",
      "Successfully built baselines\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSzSEwKdN7yu",
    "outputId": "24f2e8c4-1c88-42b2-c65e-9e388211bea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.15.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
      "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Stable Baselines only supports tensorflow 1.x for now\n",
    "%tensorflow_version 1.x\n",
    "!pip install stable-baselines[mpi]==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9fFfA-gb8oC",
    "outputId": "6f99b813-10f1-4c84-9b9f-46f4256abf47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.0.0)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfTPg6uZckCm"
   },
   "source": [
    "## Import openAI gym and define the functions used to show the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9KvZYSl6RrzD"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JLl9cs6ncAf0"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  # env = Monitor(env, './video')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7m6cSZ7PtlNs",
    "outputId": "ea4047ef-e26a-4b40-8984-198476a28797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
    "\n",
    "#check out the pacman action space!\n",
    "print(env.action_space)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "  \n",
    "    env.render()\n",
    "    \n",
    "    #your agent goes here\n",
    "    action = env.action_space.sample() \n",
    "         \n",
    "    observation, reward, done, info = env.step(action) \n",
    "   \n",
    "        \n",
    "    if done: \n",
    "      break;\n",
    "            \n",
    "env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMcGALcWeWfh"
   },
   "outputs": [],
   "source": [
    "from torch import randint\n",
    "from time import sleep\n",
    "\n",
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "reward_arr = []\n",
    "episode_count = 20\n",
    "for i in tqdm(range(episode_count)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    env.render()\n",
    "    while not done:\n",
    "        A = randint(0, env.action_space.n, (1,))\n",
    "        obs, reward, done, info = env.step(A.item())\n",
    "        rew += reward\n",
    "        sleep(0.01)\n",
    "    reward_arr.append(rew)\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL8sFLSyuMS0"
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B54NHM8kRXW-"
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmzbPJBGc8mT",
    "outputId": "804d58b9-998a-430d-bb2e-30c92a69d4a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from stable_baselines.common.policies import ActorCriticPolicy, register_policy, nature_cnn\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import A2C\n",
    "\n",
    "# Custom MLP policy of three layers of size 128 each for the actor and 2 layers of 32 for the critic,\n",
    "# with a nature_cnn feature extractor\n",
    "class CustomPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse, scale=True)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            activ = tf.nn.relu\n",
    "\n",
    "            extracted_features = nature_cnn(self.processed_obs, **kwargs)\n",
    "            extracted_features = tf.layers.flatten(extracted_features)\n",
    "\n",
    "            pi_h = extracted_features\n",
    "            for i, layer_size in enumerate([128, 128, 128]):\n",
    "                pi_h = activ(tf.layers.dense(pi_h, layer_size, name='pi_fc' + str(i)))\n",
    "            pi_latent = pi_h\n",
    "\n",
    "            vf_h = extracted_features\n",
    "            for i, layer_size in enumerate([32, 32]):\n",
    "                vf_h = activ(tf.layers.dense(vf_h, layer_size, name='vf_fc' + str(i)))\n",
    "            value_fn = tf.layers.dense(vf_h, 1, name='vf')\n",
    "            vf_latent = vf_h\n",
    "\n",
    "            self._proba_distribution, self._policy, self.q_value = \\\n",
    "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
    "\n",
    "        self._value_fn = value_fn\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        else:\n",
    "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
    "                                                   {self.obs_ph: obs})\n",
    "        return action, value, self.initial_state, neglogp\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "51iULzZQhT3r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines.ddpg.policies import LnMlpPolicy\n",
    "from stable_baselines import results_plotter\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.common.schedules import LinearSchedule\n",
    "\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "        with open('./rewards.npy', 'wb') as f:\n",
    "            np.save(f, np.array([]))\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = np.load('./rewards.npy')\n",
    "        x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "        if len(x) > 0: rewards = np.append(rewards, y[-1])\n",
    "\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          # x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        with open('./rewards.npy', 'wb') as f:\n",
    "            np.save(f, rewards)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwvVGRycuQVd"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common import set_global_seeds, make_vec_env\n",
    "from stable_baselines import ACKTR, PPO2\n",
    "\n",
    "def make_env(env_id, rank=0, seed=0, log_dir=None, wrapper_class=None, env_kwargs=None):\n",
    "    \"\"\"\n",
    "    Helper function to multiprocess training\n",
    "    and log the progress.\n",
    "    :param env_id: (str)\n",
    "    :param rank: (int)\n",
    "    :param seed: (int)\n",
    "    :param log_dir: (str)\n",
    "    :param wrapper: (type) a subclass of gym.Wrapper to wrap the original\n",
    "                    env with\n",
    "    :param env_kwargs: (Dict[str, Any]) Optional keyword argument to pass to the env constructor\n",
    "    \"\"\"\n",
    "    if log_dir is not None:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    if env_kwargs is None:\n",
    "        env_kwargs = {}\n",
    "\n",
    "    def _init():\n",
    "        set_global_seeds(seed + rank)\n",
    "        env = gym.make(env_id, **env_kwargs)\n",
    "\n",
    "        if wrapper_class:\n",
    "            env = wrapper_class(env)\n",
    "\n",
    "        env.seed(seed + rank)\n",
    "        log_file = os.path.join(log_dir, str(rank)) if log_dir is not None else None\n",
    "        env = Monitor(env, log_file)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_id = \"Skiing-v0\"\n",
    "    algo_type = 'ppo'\n",
    "    num_cpu = 4  # Number of processes to use\n",
    "    # Create the vectorized environment\n",
    "    env = SubprocVecEnv([make_env(env_id, i, log_dir=log_dir) for i in range(num_cpu)])\n",
    "\n",
    "    # model = PPO2(CustomPolicy, env, verbose=0)\n",
    "    policy = CustomPolicy\n",
    "    time_steps = 1e5\n",
    "    sched_LR = LinearSchedule(time_steps, 0.00025, 0)\n",
    "    model = PPO2(policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=sched_LR.value, vf_coef=1, max_grad_norm=0.5, lam=0.95, nminibatches=128, noptepochs=3, cliprange=0.2, cliprange_vf=None, verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None)\n",
    "\n",
    "    # Create the callback: check every N steps\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir)\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=int(time_steps), callback=callback)\n",
    "    model.save(algo_type+\"_\"+env_id)\n",
    "\n",
    "    rewards = np.load('./rewards.npy')\n",
    "    with open('./rewards_' + algo_type + '_' + env_id + '.npy', 'wb') as f:\n",
    "            np.save(f, rewards)\n",
    "\n",
    "    results_plotter.plot_results([log_dir], time_steps, results_plotter.X_TIMESTEPS, algo_type + \": \" + env_id)\n",
    "    plt.show()\n",
    "    env.close()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWIsQXY2jQJP"
   },
   "outputs": [],
   "source": [
    "y = np.load('./rewards_' + algo_type + '_' + env_id + '.npy')\n",
    "plt.plot(np.arange(y.shape[0]), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wb7BQTDttbZs"
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env_id = 'Riverraid-v0'\n",
    "algo_type = 'ppo'\n",
    "model = PPO2.load(algo_type+\"_\"+env_id)\n",
    "# model = PPO2.load('./tmp/best_model.zip')\n",
    "env = wrap_env(gym.make(env_id))\n",
    "reward_arr = []\n",
    "episode_count = 5\n",
    "for i in tqdm(range(episode_count)):\n",
    "    obs, done, rew = env.reset(), False, 0\n",
    "    env.render()\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rew += reward\n",
    "    reward_arr.append(rew)\n",
    "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVrwryxy-dtU"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reward = np.load('rewards_ppo_clip_Riverraid-v0.npy')\n",
    "df = pd.DataFrame(np.array(reward), columns = ['Rewards'])\n",
    "df['Timesteps'] = df.index\n",
    "df['Rolling_Avg_Rewards'] = df.Rewards.rolling(5000).mean()\n",
    "\n",
    "df = pd.DataFrame(np.array(reward_mod), columns = ['Rewards'])\n",
    "df['Timesteps'] = df.index\n",
    "df['Rolling_Avg_Rewards'] = df.Rewards.rolling(5000).mean()\n",
    "\n",
    "plt.plot(df['Timesteps'], df['Rewards'], alpha=0.5, label='Return')\n",
    "plt.plot(df['Timesteps'], df['Rolling_Avg_Rewards'], label='Moving Mean Return')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Riverraid-v0: PPO-Clip')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rEOW07J29VW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79pemcgb3EqW"
   },
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)\n",
    "\n",
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x): return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uur90Jf_3F1P"
   },
   "outputs": [],
   "source": [
    "# Actor module, categorical actions only\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, n_actions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsZICPk73Okb"
   },
   "outputs": [],
   "source": [
    "# Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 32),\n",
    "            activation(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKvdv-Ue3Vij"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)\n",
    "adam_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYgL2jHI43w_"
   },
   "outputs": [],
   "source": [
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)\n",
    "\n",
    "def policy_loss(old_log_prob, log_prob, advantage, eps):\n",
    "    ratio = (log_prob - old_log_prob).exp()\n",
    "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
    "    \n",
    "    m = torch.min(ratio*advantage, clipped)\n",
    "    return -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a_50SaB49pZ"
   },
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "gamma = 0.98\n",
    "eps = 0.2\n",
    "w = tensorboard.SummaryWriter()\n",
    "s = 0\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "for i in range(5):\n",
    "    prev_prob_act = None\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        s += 1\n",
    "        probs = actor(t(state))\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        prob_act = dist.log_prob(action)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action.detach().data.numpy())\n",
    "        advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))\n",
    "        \n",
    "        w.add_scalar(\"loss/advantage\", advantage, global_step=s)\n",
    "        w.add_scalar(\"actions/action_0_prob\", dist.probs[0], global_step=s)\n",
    "        w.add_scalar(\"actions/action_1_prob\", dist.probs[1], global_step=s)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if prev_prob_act:\n",
    "            actor_loss = policy_loss(prev_prob_act.detach(), prob_act, advantage.detach(), eps)\n",
    "            w.add_scalar(\"loss/actor_loss\", actor_loss, global_step=s)\n",
    "            adam_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            # clip_grad_norm_(adam_actor, max_grad_norm)\n",
    "            w.add_histogram(\"gradients/actor\",\n",
    "                             torch.cat([p.grad.view(-1) for p in actor.parameters()]), global_step=s)\n",
    "            adam_actor.step()\n",
    "\n",
    "            critic_loss = advantage.pow(2).mean()\n",
    "            w.add_scalar(\"loss/critic_loss\", critic_loss, global_step=s)\n",
    "            adam_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            # clip_grad_norm_(adam_critic, max_grad_norm)\n",
    "            w.add_histogram(\"gradients/critic\",\n",
    "                             torch.cat([p.data.view(-1) for p in critic.parameters()]), global_step=s)\n",
    "            adam_critic.step()\n",
    "        \n",
    "        prev_prob_act = prob_act\n",
    "    \n",
    "    w.add_scalar(\"reward/episode_reward\", total_reward, global_step=i)\n",
    "    episode_rewards.append(total_reward)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "csc413-final-josh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
